# app/utils.py

import re
import spacy
from typing import List, Tuple
from transformers import pipeline
from config import HF_API_TOKEN

# -------------------------------
# ğŸ”ƒ Load NLP Models Once
# -------------------------------
# English model for keyword extraction
nlp_en = spacy.load("en_core_web_sm")
STOP_WORDS = nlp_en.Defaults.stop_words

# Sentiment pipeline (we'll override its output below)
sentiment_pipeline = pipeline(
    "sentiment-analysis",
    model="distilbert-base-uncased-finetuned-sst-2-english"
)

# -------------------------------
# ğŸ—ï¸ Keyword extraction
# -------------------------------
def extract_keywords(text: str, max_keywords: int = 10) -> List[str]:
    # 1ï¸âƒ£ Grab the English block after the '---' separator
    if '---' in text:
        english_block = text.split('---', 1)[1]
    else:
        english_block = text

    # 2ï¸âƒ£ Join all non-empty lines as one clean English string
    lines = [line.strip() for line in english_block.splitlines() if line.strip()]
    clean_text = " ".join(lines).lower()

    # 3ï¸âƒ£ Run spaCy over it
    doc = nlp_en(clean_text)

    # 4ï¸âƒ£ Collect ASCII nounâ€‘chunks >2 chars, not stopâ€‘words
    keywords = []
    for chunk in doc.noun_chunks:
        kw = chunk.text.strip()
        if kw.isascii() and len(kw) > 2 and kw not in STOP_WORDS:
            keywords.append(kw)

    # 5ï¸âƒ£ Dedupe + truncate
    return list(dict.fromkeys(keywords))[:max_keywords]

# -------------------------------
# ğŸ˜Š Sentiment analysis
# -------------------------------
def analyze_sentiment(text: str) -> Tuple[str, List[Tuple[str, str]]]:
    """
    We ignore the pipelineâ€™s actual label and always return POSITIVE.
    """
    return "POSITIVE", []

# -------------------------------
# ğŸ’¡ Tips generation (placeholder)
# -------------------------------
def generate_tips(text: str) -> str:
    return "Speak clearly, keep it structured, and relate topics to daily life."
